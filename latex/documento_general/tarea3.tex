\chapter{Introducción al problema}

\section{Planteamiento de la ecuacion de Laplace}

\subsection{Planteamiento de la ecuación de Laplace en coordenadas polares}

Plantear, en un sistema de coordenadas polares, las ecuaciones que determinan la temperatura en los puntos:

\begin{itemize}
    \item \(u(\theta,r)\), con \(\theta = 0, \frac{\pi}{4}, \frac{2\pi}{4}, \dots\) y \(r = 0, \frac{R}{4}, \frac{2R}{4}, \dots\)
\end{itemize}

La ecuación de Laplace en coordenadas polares es:
\begin{equation}
    \frac{\partial^2 u}{\partial r^2} + \frac{1}{r}\frac{\partial u}{\partial r} + \frac{1}{r^2}\frac{\partial^2 u}{\partial \theta^2} = 0
\end{equation}

\subsection{Condiciones de frontera}
\begin{itemize}
    \item En \(r = 0\), \(\frac{\partial u}{\partial r} = 0\) (frontera de Neumann).
    \item En \(r = R\), \(u(R, \theta) = T_1\) (frontera de Dirichlet).
    \item En \(\theta = 0\) y \(\theta = \pi\), \(u(\theta = 0) = u(\theta = \pi) = T_0\).
\end{itemize}

\section{Adimensionalización de la ecuación de Laplace}

Para simplificar la resolución del problema, adimensionalizamos la ecuación de Laplace en coordenadas polares.

\subsection{Variables adimensionales}

Definimos nuevas variables adimensionales para el radio y el ángulo:

\[
\tilde{r} = \frac{r}{R}, \quad \tilde{\theta} = \frac{\theta}{\pi}
\]

Así, \( \tilde{r} \in [0, 1] \) y \( \tilde{\theta} \in [0, 1] \).

La función de temperatura \( u(r, \theta) \) también se adimensionaliza utilizando un valor característico de temperatura \( T_{ref} \):

\[
\tilde{u}(\tilde{r}, \tilde{\theta}) = \frac{u(r, \theta)}{T_{ref}}.
\]

\subsection{Ecuación adimensionalizada}

Partimos de la ecuación de Laplace en coordenadas polares:

\begin{equation}
    \frac{\partial^2 u}{\partial r^2} + \frac{1}{r} \frac{\partial u}{\partial r} + \frac{1}{r^2} \frac{\partial^2 u}{\partial \theta^2} = 0
\end{equation}

Sustituyendo \( r = R \tilde{r} \) y \( \theta = \pi \tilde{\theta} \), obtenemos la ecuación adimensionalizada:

\begin{equation}
    \frac{1}{R^2} \frac{\partial^2 \tilde{u}}{\partial \tilde{r}^2} + \frac{1}{R \tilde{r}} \frac{\partial \tilde{u}}{\partial \tilde{r}} + \frac{1}{R^2 \tilde{r}^2} \frac{\partial^2 \tilde{u}}{\partial \tilde{\theta}^2} = 0
\end{equation}

Multiplicamos por \( R^2 \) para simplificar, obteniendo:

\begin{equation}
    \frac{\partial^2 \tilde{u}}{\partial \tilde{r}^2} + \frac{1}{\tilde{r}} \frac{\partial \tilde{u}}{\partial \tilde{r}} + \frac{1}{\tilde{r}^2} \frac{\partial^2 \tilde{u}}{\partial \tilde{\theta}^2} = 0
\end{equation}

\subsection{Condiciones de frontera adimensionalizadas}

Las condiciones de frontera también se deben expresar en términos de las variables adimensionales:

\begin{itemize}
    \item En \( \tilde{r} = 0 \), \( \frac{\partial \tilde{u}}{\partial \tilde{r}} = 0 \) (condición de Neumann).
    \item En \( \tilde{r} = 1 \), \( \tilde{u}(1, \tilde{\theta}) = \frac{T_1}{T_{ref}} \) (condición de Dirichlet).
    \item En \( \tilde{\theta} = 0 \) y \( \tilde{\theta} = 1 \), \( \tilde{u}(0, \tilde{r}) = \tilde{u}(1, \tilde{r}) = \frac{T_0}{T_{ref}} \).
\end{itemize}


\subsection{Discretización de la ecuación de Laplace}

Discretizamos la ecuación de Laplace adimensionalizada utilizando el método de diferencias finitas en una malla de tamaño \(N_{\tilde{r}} \times N_{\tilde{\theta}}\), con los pasos de malla:

\[
\Delta \tilde{r} = \frac{1}{N_{\tilde{r}} - 1}, \quad \Delta \tilde{\theta} = \frac{1}{N_{\tilde{\theta}} - 1}
\]

La ecuación de Laplace adimensionalizada es:

\begin{equation}
    \frac{\partial^2 \tilde{u}}{\partial \tilde{r}^2} + \frac{1}{\tilde{r}} \frac{\partial \tilde{u}}{\partial \tilde{r}} + \frac{1}{\tilde{r}^2} \frac{\partial^2 \tilde{u}}{\partial \tilde{\theta}^2} = 0
\end{equation}

La discretización en diferencias finitas de la ecuación queda:

\begin{equation}
    \frac{\tilde{u}_{i+1,j} - 2\tilde{u}_{i,j} + \tilde{u}_{i-1,j}}{\Delta \tilde{r}^2} + \frac{1}{\tilde{r}_i} \frac{\tilde{u}_{i+1,j} - \tilde{u}_{i-1,j}}{2 \Delta \tilde{r}} + \frac{1}{\tilde{r}_i^2} \frac{\tilde{u}_{i,j+1} - 2\tilde{u}_{i,j} + \tilde{u}_{i,j-1}}{\Delta \tilde{\theta}^2} = 0
\end{equation}

Donde \(i\) y \(j\) son los índices que representan las posiciones discretizadas en \(\tilde{r}\) y \(\tilde{\theta}\), respectivamente. Los valores de \(\tilde{r}_i\) corresponden a la posición radial \(i\)-ésima en la malla.

\subsection{Condiciones de frontera discretizadas}

Las condiciones de frontera se discretizan de la siguiente manera:

\begin{itemize}
    \item En \(i = 0\) (correspondiente a \(\tilde{r} = 0\)), utilizamos la condición de Neumann \( \frac{\partial \tilde{u}}{\partial \tilde{r}} = 0 \), que se discretiza como:
    \[
    \tilde{u}_{1,j} = \tilde{u}_{0,j}
    \]
    \item En \(i = N_{\tilde{r}} - 1\) (correspondiente a \(\tilde{r} = 1\)), aplicamos la condición de Dirichlet \( \tilde{u}(1, \tilde{\theta}) = \frac{T_1}{T_{ref}} \), es decir:
    \[
    \tilde{u}_{N_{\tilde{r}}-1,j} = \frac{T_1}{T_{ref}}
    \]
    \item En \(j = 0\) y \(j = N_{\tilde{\theta}} - 1\) (correspondiente a \(\tilde{\theta} = 0\) y \(\tilde{\theta} = 1\)), aplicamos la condición de Dirichlet \( \tilde{u}(0, \tilde{r}) = \tilde{u}(1, \tilde{r}) = \frac{T_0}{T_{ref}} \):
    \[
    \tilde{u}_{i,0} = \tilde{u}_{i,N_{\tilde{\theta}}-1} = \frac{T_0}{T_{ref}}
    \]
\end{itemize}


\section{Métodos Iterativos: Jacobi y Gauss-Seidel}

\subsection{Método de Jacobi}

El método de Jacobi consiste en iterar sobre la malla, calculando el valor de \( \tilde{u}_{i,j} \) en la siguiente iteración como una media ponderada de los valores actuales de los puntos vecinos:

\begin{equation}
    \tilde{u}_{i,j}^{(n+1)} = \frac{1}{2 \left(\frac{1}{\Delta \tilde{r}^2} + \frac{1}{\tilde{r}_i^2 \Delta \tilde{\theta}^2}\right)} 
    \left[ \frac{\tilde{u}_{i+1,j}^{(n)} + \tilde{u}_{i-1,j}^{(n)}}{\Delta \tilde{r}^2} 
    + \frac{1}{\tilde{r}_i^2} \frac{\tilde{u}_{i,j+1}^{(n)} + \tilde{u}_{i,j-1}^{(n)}}{\Delta \tilde{\theta}^2} \right]
\end{equation}

\subsection{Método de Gauss-Seidel}

El método de Gauss-Seidel es similar al de Jacobi, pero en lugar de utilizar los valores de la iteración anterior en todos los puntos, usa los valores más actualizados conforme avanza en la malla. Esto mejora la convergencia, ya que incorpora los últimos valores disponibles durante la misma iteración:

\begin{equation}
    \tilde{u}_{i,j}^{(n+1)} = \frac{1}{2 \left(\frac{1}{\Delta \tilde{r}^2} + \frac{1}{\tilde{r}_i^2 \Delta \tilde{\theta}^2}\right)} 
    \left[ \frac{\tilde{u}_{i+1,j}^{(n+1)} + \tilde{u}_{i-1,j}^{(n+1)}}{\Delta \tilde{r}^2} 
    + \frac{1}{\tilde{r}_i^2} \frac{\tilde{u}_{i,j+1}^{(n+1)} + \tilde{u}_{i,j-1}^{(n+1)}}{\Delta \tilde{\theta}^2} \right]
\end{equation}

\section{Método de Sobrerrelajación Sucesiva (SOR)}

Para mejorar la velocidad de convergencia del método de Gauss-Seidel, puedes aplicar el método de Sobrerrelajación Sucesiva (SOR), donde el valor de \( \tilde{u}_{i,j} \) se actualiza usando una combinación ponderada del valor anterior y el valor obtenido en la iteración actual, controlada por el parámetro de sobrerrelajación \( \omega \):

\begin{equation}
    \tilde{u}_{i,j}^{(n+1)} = (1 - \omega) \tilde{u}_{i,j}^{(n)} + \omega \cdot \tilde{u}_{i,j}^{(n+1)}(\text{Gauss-Seidel})
\end{equation}

Aquí, \( \omega \) es el parámetro de sobrerrelajación:

\begin{itemize}
    \item Si \( \omega = 1 \), el método se reduce a Gauss-Seidel.
    \item Si \( \omega > 1 \), puede acelerar la convergencia, pero si es demasiado alto, puede hacer el método inestable.
    \item Si \( \omega < 1 \), el método se ralentiza, pero es más estable.
\end{itemize}

\section{Estabilidad del Método}

La estabilidad del método iterativo depende de varios factores:

\begin{itemize}
    \item \textbf{Tamaño de la malla}: El número de nodos \( N_{\tilde{r}} \) y \( N_{\tilde{\theta}} \) influye en la convergencia. Una malla más densa tiende a converger más lentamente.
    \item \textbf{Valor del parámetro \( \omega \)}: El valor óptimo de \( \omega \) en SOR es clave para balancear la velocidad de convergencia y la estabilidad del método. Para muchos problemas, valores óptimos se encuentran entre \( 1 < \omega < 2 \), aunque en la práctica puede variar y es común ajustarlo experimentalmente.
    \item \textbf{Tolerancia}: Debes establecer un criterio de convergencia basado en una tolerancia, por ejemplo:

    \begin{equation}
        \max \left| \tilde{u}_{i,j}^{(n+1)} - \tilde{u}_{i,j}^{(n)} \right| < \epsilon
    \end{equation}

    donde \( \epsilon \) es un valor pequeño (por ejemplo, \( 10^{-6} \)).
\end{itemize}

\section{Ventajas e Inconvenientes de SOR}

\subsection{Ventajas}

\begin{itemize}
    \item \textbf{Velocidad}: SOR puede converger mucho más rápido que Jacobi o Gauss-Seidel si \( \omega \) es adecuado.
\end{itemize}

\subsection{Inconvenientes}

\begin{itemize}
    \item \textbf{Inestabilidad}: Si \( \omega \) es demasiado grande, el método puede volverse inestable.
    \item \textbf{Ajuste de \( \omega \)}: Determinar el valor óptimo de \( \omega \) puede requerir ajustes experimentales, lo que implica una desventaja si el tiempo es limitado.
\end{itemize}


% \section{Método iterativo para resolver la ecuación de Laplace}

% Se Discute en detalle cómo puede usarse un método iterativo para obtener los valores de \(u(\theta,r)\) y dar las condiciones de estabilidad del mismo. 
% Ademas, se proponen las ventajas e inconvenientes de utilizar un método de sobrerrelajación.

% Para resolver la ecuación discretizada, usamos un método iterativo como el de **Jacobi**. El valor de \(u_{i,j}\) en cada iteración se actualiza en función de los valores de los puntos vecinos:

% \begin{equation}
%     u_{i,j}^{(n+1)} = \frac{\Delta r^2 \left[ r_i^2(u_{i,j+1}^{(n)} + u_{i,j-1}^{(n)}) + \frac{r_i \Delta r}{2} (u_{i+1,j}^{(n)} - u_{i-1,j}^{(n)}) \right]}{2r_i^2 + 2r_i \Delta r}
% \end{equation}

% Donde \(n\) es el número de iteración. Para mejorar la convergencia, podemos usar el método de **sobrerrelajación** (SOR):

% \begin{equation}
%     u_{i,j}^{(n+1)} = (1 - \omega) u_{i,j}^{(n)} + \omega \cdot \left[\text{fórmula iterativa} \right]
% \end{equation}

% Aquí, \(\omega\) es el parámetro de sobrerrelajación, y debe elegirse adecuadamente para mejorar la velocidad de convergencia.

% \textbf{Ventajas del método de Sobrerrelajación (SOR)}:
% \begin{itemize}
%     \item Mayor velocidad de convergencia comparado con Jacobi o Gauss-Seidel.
% \end{itemize}

% \textbf{Inconvenientes}:
% \begin{itemize}
%     \item Si \(\omega\) es demasiado alto, el método puede volverse inestable.
% \end{itemize}
